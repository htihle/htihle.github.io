---
layout: default
title: Blog
---

# Blog

I write about AI capabilities, benchmarks, and research on [LessWrong](https://www.lesswrong.com/).

<div class="blog-grid">
  <a href="https://www.lesswrong.com/posts/fpdjaF7kdtcvmhhfE/can-llms-coordinate-a-simple-schelling-point-experiment" class="blog-card">
    <img class="blog-card-img" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/w_360,c_fill,ar_1.5,g_auto,q_auto/SocialPreview/outgpsgnrekp2zoinmob" alt="">
    <div class="blog-card-content">
      <span class="blog-card-title">Can LLMs Coordinate? A Simple Schelling Point Experiment</span>
      <span class="blog-card-desc">Testing whether five advanced AI models can coordinate on matching answers to 75 prompts without communication. Models excelled on concrete prompts like "a number between 1 and 10" (achieving perfect agreement on "7") but struggled with more abstract requests.</span>
    </div>
  </a>
  <a href="https://www.lesswrong.com/posts/ifSBamvobbyB9KWjK/inference-costs-for-hard-coding-tasks-halve-roughly-every" class="blog-card">
    <img class="blog-card-img" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/w_360,c_fill,ar_1.5,g_auto,q_auto/v1/mirroredImages/ifSBamvobbyB9KWjK/jsdeg4ekzukyhwfikitk" alt="">
    <div class="blog-card-content">
      <span class="blog-card-title">Inference costs for hard coding tasks halve roughly every two months</span>
      <span class="blog-card-desc">Analysis of declining AI inference costs using WeirdML and Aider Polyglot benchmark data. The research shows that the cost to achieve a certain score halves roughly every two monthsâ€”tasks that once cost dollars with GPT-4 can now be completed for fractions of a cent.</span>
    </div>
  </a>
  <a href="https://www.lesswrong.com/posts/NLnGRDRXATW2pqXuE/is-the-gap-between-open-and-closed-models-growing-evidence" class="blog-card">
    <img class="blog-card-img" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/w_360,c_fill,ar_1.5,g_auto,q_auto/v1/mirroredImages/NLnGRDRXATW2pqXuE/h17ctk0ipdfss6rttzr6" alt="">
    <div class="blog-card-content">
      <span class="blog-card-title">Is the gap between open and closed models growing? Evidence from WeirdML</span>
      <span class="blog-card-desc">Analysis from the WeirdML benchmark showing that the gap between open and closed models is not shrinking over time. Closed-source models like OpenAI's o1 and o3 maintain a substantial lead over open-weights alternatives even months after their release.</span>
    </div>
  </a>
  <a href="https://www.lesswrong.com/posts/LfQCzph7rc2vxpweS/introducing-the-weirdml-benchmark" class="blog-card">
    <img class="blog-card-img" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/w_360,c_fill,ar_1.5,g_auto,q_auto/v1/mirroredImages/LfQCzph7rc2vxpweS/jjmth83z1frhtduep3ir" alt="" loading="lazy">
    <div class="blog-card-content">
      <span class="blog-card-title">Introducing the WeirdML Benchmark</span>
      <span class="blog-card-desc">Introducing a benchmark that evaluates how well LLMs perform machine learning on novel datasets. Tests the ability to understand data properties, generate working PyTorch code, and improve solutions through iterative feedback over five rounds.</span>
    </div>
  </a>
  <a href="https://www.lesswrong.com/posts/vLB4tkLWYmQrJstCA/o1-preview-is-pretty-good-at-doing-ml-on-an-unknown-dataset" class="blog-card">
    <img class="blog-card-img" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/w_360,c_fill,ar_1.5,g_auto,q_auto/v1/mirroredImages/Fr6eJkjYWG9Mw6XQc/ahubz59kswgozihheyov" alt="" loading="lazy">
    <div class="blog-card-content">
      <span class="blog-card-title">o1-preview is pretty good at doing ML on an unknown dataset</span>
      <span class="blog-card-desc">Evaluating OpenAI's o1-preview on a shape classification challenge from 2D point data. The model achieved 77% accuracy on its fourth submission, significantly outperforming GPT-4o and Claude, showing a major advancement in handling complex, unfamiliar tasks.</span>
    </div>
  </a>
  <a href="https://www.lesswrong.com/posts/Fr6eJkjYWG9Mw6XQc/how-good-are-llms-at-doing-ml-on-an-unknown-dataset" class="blog-card">
    <img class="blog-card-img" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/w_360,c_fill,ar_1.5,g_auto,q_auto/v1/mirroredImages/Fr6eJkjYWG9Mw6XQc/ahubz59kswgozihheyov" alt="" loading="lazy">
    <div class="blog-card-content">
      <span class="blog-card-title">How good are LLMs at doing ML on an unknown dataset?</span>
      <span class="blog-card-desc">The original exploration that led to WeirdML. Challenged GPT-4o, Claude Sonnet 3.5, and Gemini to develop classifiers for geometric shapes from 2D point clouds. Found unreliable performance and fundamental gaps in ML reasoning capabilities.</span>
    </div>
  </a>
</div>
